{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given N battlefields and S soldiers, the number of possible distributions of soldiers is \n",
    "$${N + S - 1 \\choose N - 1}$$\n",
    "By partitioning on the number of soldiers on the first battlefield and letting $s$ be the number of soldiers in the remaining $N-1$ battlefields, we note\n",
    "$$\n",
    "{N + S - 1 \\choose N - 1} = \\sum_{s = 0}^S {N + s - 2 \\choose N - 2}\n",
    "$$\n",
    "We can hence extract the number of soldiers on the first battlefield. Then we proceed recursively. For example, for $(N, S) = (3,4)$, we have $15$ possibilities. Let $i$ denote the index.\n",
    "\n",
    "\n",
    "If $i < {N - 2 \\choose N - 2} = 1$, we know the first battlefield contains all soldiers.\n",
    "\n",
    "Elif $i < 1 + {N + 1 - 2 \\choose N - 2} = 1 + 2$, we know the first battlefield all but one soldiers.\n",
    "\n",
    "Elif $i < 1 + 2 + {N + 2 - 2 \\choose N - 2} = 1 + 2 + 3$, we know the first battlefield all but two soldiers.\n",
    "\n",
    "etc. Once we know the amount of soldiers in the first battlefield, we call the function on $(N-1, S-s)$.\n",
    "\n",
    "For example, if $i=4$, we see that the first battlefield has 2 solders. We then set $i = i- 3=1$ and run the function on $(2, 2)$ so we know that the second battlefield has 1 soldier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infrastructure import find_CE\n",
    "from Blotto_infra import *\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0, 10,  0,  0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def soldier_dist(i, N, S, out):\n",
    "    if S == 0:\n",
    "        out += [0] * N\n",
    "        return np.array(out)\n",
    "    if N == 1:\n",
    "        out.append(S)\n",
    "        return np.array(out)\n",
    "\n",
    "    temp = 1\n",
    "    for s in range(S + 1):\n",
    "        if s >= 1:\n",
    "            temp = (temp * (N + s - 2)) // s\n",
    "        if temp > i:\n",
    "            out.append(S - s)\n",
    "            return soldier_dist(i, N - 1, s, out)\n",
    "        else:\n",
    "            i -= temp\n",
    "soldier_dist(935, 5, 10, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viable strategies for agent 0:\n",
      "\t Probability of playing [5 2 0] is 0.024.\n",
      "\t Probability of playing [5 0 2] is 0.017.\n",
      "\t Probability of playing [4 3 0] is 0.061.\n",
      "\t Probability of playing [4 2 1] is 0.062.\n",
      "\t Probability of playing [4 1 2] is 0.077.\n",
      "\t Probability of playing [4 0 3] is 0.076.\n",
      "\t Probability of playing [3 4 0] is 0.062.\n",
      "\t Probability of playing [3 0 4] is 0.062.\n",
      "\t Probability of playing [2 5 0] is 0.025.\n",
      "\t Probability of playing [2 4 1] is 0.089.\n",
      "\t Probability of playing [2 1 4] is 0.077.\n",
      "\t Probability of playing [2 0 5] is 0.013.\n",
      "\t Probability of playing [1 4 2] is 0.078.\n",
      "\t Probability of playing [1 2 4] is 0.088.\n",
      "\t Probability of playing [0 5 2] is 0.021.\n",
      "\t Probability of playing [0 4 3] is 0.053.\n",
      "\t Probability of playing [0 3 4] is 0.069.\n",
      "\t Probability of playing [0 2 5] is 0.029.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Viable strategies for agent 1:\n",
      "\t Probability of playing [5 2 0] is 0.033.\n",
      "\t Probability of playing [5 0 2] is 0.021.\n",
      "\t Probability of playing [4 3 0] is 0.068.\n",
      "\t Probability of playing [4 2 1] is 0.061.\n",
      "\t Probability of playing [4 1 2] is 0.075.\n",
      "\t Probability of playing [4 0 3] is 0.075.\n",
      "\t Probability of playing [3 4 0] is 0.057.\n",
      "\t Probability of playing [3 0 4] is 0.067.\n",
      "\t Probability of playing [2 5 0] is 0.028.\n",
      "\t Probability of playing [2 4 1] is 0.08.\n",
      "\t Probability of playing [2 1 4] is 0.074.\n",
      "\t Probability of playing [2 0 5] is 0.013.\n",
      "\t Probability of playing [1 4 2] is 0.079.\n",
      "\t Probability of playing [1 2 4] is 0.07.\n",
      "\t Probability of playing [0 5 2] is 0.024.\n",
      "\t Probability of playing [0 4 3] is 0.058.\n",
      "\t Probability of playing [0 3 4] is 0.066.\n",
      "\t Probability of playing [0 2 5] is 0.038.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Single_Evaluation(Blotto(3, (7, 7)), 10000).viable_strategies(eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<infrastructure.Regret_Minimisation_Agent at 0x2a5b4a0bf10>,\n",
       " <infrastructure.Regret_Minimisation_Agent at 0x2a5b4a0ada0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_CE(Blotto(3, (7, 7)), iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soldier_dist(6, 3, 7, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m e \u001b[39m=\u001b[39m Evaluation(Blotto(\u001b[39m3\u001b[39;49m,\u001b[39m5\u001b[39;49m), \u001b[39m1000000\u001b[39;49m, \u001b[39m10\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m e\u001b[39m.\u001b[39maverage_strategy\n",
      "File \u001b[1;32mc:\\Users\\Prog\\Documents\\Regret-learning\\infrastructure.py:145\u001b[0m, in \u001b[0;36mEvaluation.__init__\u001b[1;34m(self, game, rounds, sample_size)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_size \u001b[39m=\u001b[39m sample_size\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds \u001b[39m=\u001b[39m rounds\n\u001b[1;32m--> 145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([find_Nash(game, iterations\u001b[39m=\u001b[39mrounds) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)])\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage_strategy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes, axis\u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Prog\\Documents\\Regret-learning\\infrastructure.py:145\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_size \u001b[39m=\u001b[39m sample_size\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds \u001b[39m=\u001b[39m rounds\n\u001b[1;32m--> 145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([find_Nash(game, iterations\u001b[39m=\u001b[39;49mrounds) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sample_size)])\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage_strategy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes, axis\u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutcomes, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Prog\\Documents\\Regret-learning\\infrastructure.py:160\u001b[0m, in \u001b[0;36mfind_Nash\u001b[1;34m(game, iterations)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_Nash\u001b[39m(game: Game, iterations\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m):\n\u001b[0;32m    159\u001b[0m     agents \u001b[39m=\u001b[39m [Regret_Minimisation_Agent(game, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(game\u001b[39m.\u001b[39mplayer_count)]\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m Trainer(game, agents)\u001b[39m.\u001b[39;49mtrain(iterations, \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Prog\\Documents\\Regret-learning\\infrastructure.py:131\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, n, out)\u001b[0m\n\u001b[0;32m    129\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(agent\u001b[39m.\u001b[39maction() \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents)\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n\u001b[1;32m--> 131\u001b[0m         agent\u001b[39m.\u001b[39;49mupdate(action)\n\u001b[0;32m    132\u001b[0m strats \u001b[39m=\u001b[39m []\n\u001b[0;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents:\n",
      "File \u001b[1;32mc:\\Users\\Prog\\Documents\\Regret-learning\\infrastructure.py:105\u001b[0m, in \u001b[0;36mRegret_Minimisation_Agent.update\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    102\u001b[0m chosen_utility \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mupdate(actions)\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_count):\n\u001b[1;32m--> 105\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(actions)\n\u001b[0;32m    106\u001b[0m     temp[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex] \u001b[39m=\u001b[39m i\n\u001b[0;32m    107\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(temp)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e = Evaluation(Blotto(3,5), 1000000, 10)\n",
    "e.average_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.    , 0.    , 0.112 , 0.1125, 0.109 , 0.1086, 0.    ,\n",
       "       0.    , 0.1115, 0.    , 0.1132, 0.    , 0.113 , 0.    , 0.    ,\n",
       "       0.    , 0.1115, 0.1087, 0.    , 0.    ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(e.average_strategy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 0.    , 0.    , 0.0104, 0.0096, 0.011 , 0.0112, 0.    ,\n",
       "        0.    , 0.008 , 0.    , 0.0086, 0.    , 0.0078, 0.    , 0.    ,\n",
       "        0.    , 0.0074, 0.011 , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.015 , 0.0104, 0.0098, 0.0116, 0.    ,\n",
       "        0.    , 0.0114, 0.    , 0.0056, 0.    , 0.0084, 0.    , 0.    ,\n",
       "        0.    , 0.0076, 0.0116, 0.    , 0.    ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(e.sds, 4) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of Agent 0 is -3526\n",
      "[0.    0.    0.    0.109 0.112 0.112 0.112 0.    0.    0.109 0.    0.112\n",
      " 0.    0.112 0.    0.    0.    0.109 0.112 0.    0.   ]\n",
      "Utility of Agent 1 is 3526\n",
      "[0.    0.    0.    0.102 0.12  0.108 0.111 0.    0.    0.108 0.    0.118\n",
      " 0.    0.114 0.    0.    0.    0.104 0.115 0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "find_Nash(Blotto(3,5), iterations=15000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=3\n",
    "S=3\n",
    "comb(N + S -1, N-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6]],\n",
       "\n",
       "       [[2, 3, 4],\n",
       "        [6, 7, 8]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr =np.array([np.array([np.array([1,2,3]), np.array([4,5,6])]), np.array([np.array([1,2,3]) + 1, np.array([4,5,6]) + 2])] )\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5],\n",
       "       [1. , 1. , 1. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb7e7e116cf573f9806de94c4b2f308bcfee17df1b0d668c2e26b972481ef263"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
